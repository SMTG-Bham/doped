name: Tests

on:
  workflow_dispatch:

  push:
    branches:
      - '*'  # all branches

jobs:
  test:
    strategy:
      fail-fast: false

      matrix:
        os: [ ubuntu-latest, macos-14 ]
        python-version: [ '3.10', '3.13' ]  # lowest and highest supported Python versions

        # if durations files present, pytest-split will use them to intelligently distribute tests
        split: [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 ]

    env:
      MPLBACKEND: Agg  # non-interactive backend for matplotlib

    runs-on: ${{ matrix.os }}

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[tests]

      - name: Test
        run:  |  # update durations file with `pytest -vv -m "not mpl_image_compare" tests --store-durations --durations-path tests/.pytest_split_durations_non_plotting`
          pytest -vv -m "not mpl_image_compare" --splits 10 --group ${{ matrix.split }} --durations-path tests/.pytest_split_durations_non_plotting tests

      - name: Plotting Tests
        if: always()  # run even if non-plotting tests fail
        id: plotting_tests  # Add an ID to this step for reference
        env:
          MPLBACKEND: Agg  # non-interactive backend for matplotlib
        run: |  # update durations file with `pytest -vv -m "mpl_image_compare" tests --store-durations --durations-path tests/.pytest_split_durations_plotting`
          set +e
          pytest -vv --mpl -m "mpl_image_compare" \
            --splits 10 --group ${{ matrix.split }} \
            --durations-path tests/.pytest_split_durations_plotting tests
          ec=$?
          set -e
          if [ $ec -eq 5 ]; then
            echo "ðŸ”¹ No tests in this split (exit code 5) â€” treating as success"
            exit 0
          else
            exit $ec
          fi
          
      - name: Generate GH Actions test plots
        if: failure() && steps.plotting_tests.outcome == 'failure'  # Run only if plotting tests fail
        env:
          MPLBACKEND: Agg  # non-interactive backend for matplotlib
        run: |  # Generate the test plots in case there were any failures:
          set +e
          pytest -vv --mpl-generate-path=tests/remote_baseline -m "mpl_image_compare" \
            --splits 10 --group ${{ matrix.split }} \
            --durations-path tests/.pytest_split_durations_plotting tests
          ec=$?
          set -e
          if [ $ec -eq 5 ]; then
            echo "ðŸ”¹ No tests in this split (exit code 5) â€” treating as success"
            exit 0
          else
            exit $ec
          fi

      # Upload test plots
      - name: Archive test plots
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: output-plots
          path: tests/remote_baseline
